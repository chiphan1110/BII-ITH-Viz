{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, roc_auc_score\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_val_predict\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m f1_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_dicts(dir):\n",
    "    json_files = {}\n",
    "    sorted_filenames = sorted(os.listdir(dir), key= lambda x: len(x))\n",
    "    for filename in sorted_filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            key_name = filename.replace('.json', '')\n",
    "            file_path = os.path.join(dir, filename)\n",
    "            json_files[key_name] = file_path\n",
    "    \n",
    "    return json_files       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FGA': 'data\\\\FGA.json',\n",
       " 'Purity': 'data\\\\Purity.json',\n",
       " 'Staging': 'data\\\\Staging.json',\n",
       " 'Subtyping': 'data\\\\Subtyping.json',\n",
       " 'Purity-FGA': 'data\\\\Purity-FGA.json',\n",
       " 'Staging-FGA': 'data\\\\Staging-FGA.json',\n",
       " 'Subtyping-FGA': 'data\\\\Subtyping-FGA.json',\n",
       " 'Staging-Purity': 'data\\\\Staging-Purity.json',\n",
       " 'Subtyping-Purity': 'data\\\\Subtyping-Purity.json',\n",
       " 'Subtyping-Staging': 'data\\\\Subtyping-Staging.json',\n",
       " 'Staging-Purity-FGA': 'data\\\\Staging-Purity-FGA.json',\n",
       " 'Subtyping-Purity-FGA': 'data\\\\Subtyping-Purity-FGA.json',\n",
       " 'Subtyping-Staging-FGA': 'data\\\\Subtyping-Staging-FGA.json',\n",
       " 'Subtyping-Staging-Purity': 'data\\\\Subtyping-Staging-Purity.json',\n",
       " 'Subtyping-Staging-Purity-FGA': 'data\\\\Subtyping-Staging-Purity-FGA.json'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dir = 'data'\n",
    "json_files = generate_file_dicts(json_dir)\n",
    "json_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_signed_error(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        error = round((pred - label), 3)\n",
    "        errors.append(error)   \n",
    "    return errors\n",
    "\n",
    "def calculate_abs_error(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    abs_error = []\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        error = round(abs(pred - label), 3)\n",
    "        abs_error.append(error)   \n",
    "    return abs_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_task(classifiers, error_type):\n",
    "    purity_errors = {}\n",
    "    fga_errors = {}\n",
    "    \n",
    "    if (error_type == 'abs'):\n",
    "        for classifier_name, json_file in classifiers.items():\n",
    "            if 'Purity' in classifier_name:\n",
    "                purity_errors[classifier_name] = calculate_abs_error(json_file, \"purity\")\n",
    "            if 'FGA' in classifier_name:\n",
    "                fga_errors[classifier_name] = calculate_abs_error(json_file, \"FRACTION_GENOME_ALTERED\")\n",
    "    \n",
    "    elif (error_type == 'signed'):\n",
    "        for classifier_name, json_file in classifiers.items():\n",
    "            if 'Purity' in classifier_name:\n",
    "                purity_errors[classifier_name] = calculate_signed_error(json_file, \"purity\")\n",
    "            if 'FGA' in classifier_name:\n",
    "                fga_errors[classifier_name] = calculate_signed_error(json_file, \"FRACTION_GENOME_ALTERED\")    \n",
    "\n",
    "    return pd.DataFrame(purity_errors), pd.DataFrame(fga_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_errors, fga_errors = numerical_task(json_files, 'abs')\n",
    "purity_errors.to_csv('calculation/abs-errors/purity.csv', index_label='Sample')\n",
    "fga_errors.to_csv('calculation/abs-errors/fga.csv', index_label='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_errors, fga_errors = numerical_task(json_files, 'signed')\n",
    "purity_errors.to_csv('calculation/signed-errors/purity.csv', index_label='Sample')\n",
    "fga_errors.to_csv('calculation/signed-errors/fga.csv', index_label='Sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_accuracy(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "    unique_labels = set()\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        labels.append(label)\n",
    "        preds.append(pred)   \n",
    "        unique_labels.add(label)\n",
    "    \n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_task_staging(classifiers):\n",
    "    staging_labels = []\n",
    "    staging_preds = []\n",
    "    classification_reports = {}\n",
    "    confusion_matrices = {}\n",
    "    results = []\n",
    "\n",
    "    for classifier_name, json_file in classifiers.items():\n",
    "        if 'Staging' in classifier_name:\n",
    "            staging_labels, staging_preds = report_accuracy(json_file, \"AJCC_PATHOLOGIC_TUMOR_STAGE_reduced\")\n",
    "            \n",
    "            classification_reports[classifier_name] = classification_report(staging_labels, staging_preds, target_names=['Early Stage', 'Late Stage'])\n",
    "            confusion_matrices[classifier_name] = confusion_matrix(staging_labels, staging_preds, labels=['Early Stage', 'Late Stage']).tolist()\n",
    "            print(confusion_matrix(staging_labels, staging_preds))\n",
    "\n",
    "            f1_macro = f1_score(staging_labels, staging_preds, average=\"macro\")\n",
    "            f1_scores_per_class = f1_score(staging_labels, staging_preds, average=None)\n",
    "            f1_std_dev = np.std(f1_scores_per_class)\n",
    "            precision = precision_score(staging_labels, staging_preds, average=\"weighted\")\n",
    "            recall = recall_score(staging_labels, staging_preds, average='weighted')\n",
    "\n",
    "            results.append({\n",
    "                'Model': classifier_name,  \n",
    "                'F1 Score': round(f1_macro, 2),\n",
    "                'F1 StdDev': round(f1_std_dev, 2), \n",
    "                'Precision': round(precision, 2),\n",
    "                'Recall': round(recall, 2)\n",
    "            })\n",
    "\n",
    "    value_counts = Counter(staging_labels)\n",
    "    for value, count in value_counts.items():\n",
    "        print(f'{value}: {count}')\n",
    "\n",
    "    print()\n",
    "        \n",
    "    return classification_reports, confusion_matrices, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[241  17]\n",
      " [ 43   6]]\n",
      "[[257   1]\n",
      " [ 47   2]]\n",
      "[[233  25]\n",
      " [ 40   9]]\n",
      "[[227  31]\n",
      " [ 42   7]]\n",
      "[[249   9]\n",
      " [ 47   2]]\n",
      "[[229  29]\n",
      " [ 41   8]]\n",
      "[[243  15]\n",
      " [ 45   4]]\n",
      "[[239  19]\n",
      " [ 41   8]]\n",
      "Early Stage: 258\n",
      "Late Stage: 49\n",
      "\n",
      "Staging\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.85      0.93      0.89       258\n",
      "  Late Stage       0.26      0.12      0.17        49\n",
      "\n",
      "    accuracy                           0.80       307\n",
      "   macro avg       0.55      0.53      0.53       307\n",
      "weighted avg       0.75      0.80      0.77       307\n",
      "\n",
      "[[241, 17], [43, 6]]\n",
      "Staging-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.85      1.00      0.91       258\n",
      "  Late Stage       0.67      0.04      0.08        49\n",
      "\n",
      "    accuracy                           0.84       307\n",
      "   macro avg       0.76      0.52      0.50       307\n",
      "weighted avg       0.82      0.84      0.78       307\n",
      "\n",
      "[[257, 1], [47, 2]]\n",
      "Staging-Purity\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.85      0.90      0.88       258\n",
      "  Late Stage       0.26      0.18      0.22        49\n",
      "\n",
      "    accuracy                           0.79       307\n",
      "   macro avg       0.56      0.54      0.55       307\n",
      "weighted avg       0.76      0.79      0.77       307\n",
      "\n",
      "[[233, 25], [40, 9]]\n",
      "Subtyping-Staging\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.84      0.88      0.86       258\n",
      "  Late Stage       0.18      0.14      0.16        49\n",
      "\n",
      "    accuracy                           0.76       307\n",
      "   macro avg       0.51      0.51      0.51       307\n",
      "weighted avg       0.74      0.76      0.75       307\n",
      "\n",
      "[[227, 31], [42, 7]]\n",
      "Staging-Purity-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.84      0.97      0.90       258\n",
      "  Late Stage       0.18      0.04      0.07        49\n",
      "\n",
      "    accuracy                           0.82       307\n",
      "   macro avg       0.51      0.50      0.48       307\n",
      "weighted avg       0.74      0.82      0.77       307\n",
      "\n",
      "[[249, 9], [47, 2]]\n",
      "Subtyping-Staging-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.85      0.89      0.87       258\n",
      "  Late Stage       0.22      0.16      0.19        49\n",
      "\n",
      "    accuracy                           0.77       307\n",
      "   macro avg       0.53      0.53      0.53       307\n",
      "weighted avg       0.75      0.77      0.76       307\n",
      "\n",
      "[[229, 29], [41, 8]]\n",
      "Subtyping-Staging-Purity\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.84      0.94      0.89       258\n",
      "  Late Stage       0.21      0.08      0.12        49\n",
      "\n",
      "    accuracy                           0.80       307\n",
      "   macro avg       0.53      0.51      0.50       307\n",
      "weighted avg       0.74      0.80      0.77       307\n",
      "\n",
      "[[243, 15], [45, 4]]\n",
      "Subtyping-Staging-Purity-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Early Stage       0.85      0.93      0.89       258\n",
      "  Late Stage       0.30      0.16      0.21        49\n",
      "\n",
      "    accuracy                           0.80       307\n",
      "   macro avg       0.57      0.54      0.55       307\n",
      "weighted avg       0.76      0.80      0.78       307\n",
      "\n",
      "[[239, 19], [41, 8]]\n"
     ]
    }
   ],
   "source": [
    "staging_reports, staging_matrices, results = categorical_task_staging(json_files)\n",
    "\n",
    "staging_csv = 'calculation\\classification\\staging.csv'\n",
    "with open(staging_csv, mode='w', newline='\\n') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Model', 'F1 Score', 'F1 StdDev', 'Precision', 'Recall', 'FPR', 'TPR', 'Threshold', 'AUC'])\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "    \n",
    "json_file = 'calculation\\classification\\confusion_staging.json'\n",
    "with open(json_file, \"w\") as file:\n",
    "    for i, (model_name, matrix) in enumerate(staging_matrices.items()):\n",
    "        if i > 0:\n",
    "            file.write(\"\\n\")\n",
    "        json.dump({model_name: matrix}, file)\n",
    "\n",
    "\n",
    "\n",
    "for classifier_name, json_file in json_files.items():\n",
    "    if 'Staging' in classifier_name:\n",
    "        print(classifier_name + \"\\n\")\n",
    "        print(staging_reports[classifier_name])\n",
    "        print(staging_matrices[classifier_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_task_subtyping(classifiers):\n",
    "    subtyping_labels = []\n",
    "    subtyping_preds = []\n",
    "    classification_reports = {}\n",
    "    confusion_matrices = {}\n",
    "    results = []\n",
    "    for classifier_name, json_file in classifiers.items():\n",
    "        if 'Subtyping' in classifier_name:\n",
    "            subtyping_labels, subtyping_preds = report_accuracy(json_file, \"lung-cancer-subtyping\")\n",
    "            classification_reports[classifier_name] = classification_report(subtyping_labels, subtyping_preds, target_names=['normal', 'luad', 'lusc'])\n",
    "            confusion_matrices[classifier_name] = confusion_matrix(subtyping_labels, subtyping_preds, labels=['normal', 'luad', 'lusc']).tolist()\n",
    "\n",
    "            f1_macro = f1_score(subtyping_labels, subtyping_preds, average=\"macro\")\n",
    "            f1_scores_per_class = f1_score(subtyping_labels, subtyping_preds, average=None)\n",
    "            f1_std_dev = np.std(f1_scores_per_class)\n",
    "            precision = precision_score(subtyping_labels, subtyping_preds, average=\"weighted\")\n",
    "            recall = recall_score(subtyping_labels, subtyping_preds, average='weighted')\n",
    "\n",
    "            results.append({\n",
    "                'Model': classifier_name,  \n",
    "                'F1 Score': round(f1_macro, 2),\n",
    "                'F1 StdDev': round(f1_std_dev, 2), \n",
    "                'Precision': round(precision, 2),\n",
    "                'Recall': round(recall, 2)\n",
    "            })\n",
    "\n",
    "    value_counts = Counter(subtyping_labels)\n",
    "    for value, count in value_counts.items():\n",
    "        print(f'{value}: {count}')\n",
    "\n",
    "    print()       \n",
    "\n",
    "    return classification_reports, confusion_matrices, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lusc: 166\n",
      "normal: 92\n",
      "luad: 146\n",
      "\n",
      "Subtyping\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.82      0.77       146\n",
      "        luad       0.85      0.72      0.78       166\n",
      "        lusc       0.88      0.95      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[87, 4, 1], [7, 119, 20], [5, 42, 119]]\n",
      "Subtyping-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.71      0.82      0.76       146\n",
      "        luad       0.86      0.71      0.78       166\n",
      "        lusc       0.87      0.93      0.90        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.81       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[86, 5, 1], [8, 120, 18], [5, 43, 118]]\n",
      "Subtyping-Purity\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.77      0.82      0.79       146\n",
      "        luad       0.87      0.77      0.82       166\n",
      "        lusc       0.86      0.96      0.91        92\n",
      "\n",
      "    accuracy                           0.83       404\n",
      "   macro avg       0.83      0.85      0.84       404\n",
      "weighted avg       0.83      0.83      0.83       404\n",
      "\n",
      "[[88, 3, 1], [9, 119, 18], [5, 33, 128]]\n",
      "Subtyping-Staging\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.77      0.75       146\n",
      "        luad       0.82      0.76      0.79       166\n",
      "        lusc       0.89      0.92      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.82       404\n",
      "weighted avg       0.80      0.80      0.80       404\n",
      "\n",
      "[[85, 7, 0], [6, 113, 27], [4, 36, 126]]\n",
      "Subtyping-Purity-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.79      0.75      0.77       146\n",
      "        luad       0.81      0.83      0.82       166\n",
      "        lusc       0.89      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.82       404\n",
      "   macro avg       0.83      0.84      0.83       404\n",
      "weighted avg       0.82      0.82      0.82       404\n",
      "\n",
      "[[86, 5, 1], [6, 109, 31], [5, 24, 137]]\n",
      "Subtyping-Staging-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.73      0.82      0.77       146\n",
      "        luad       0.83      0.76      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.83      0.83      0.83       404\n",
      "weighted avg       0.82      0.81      0.81       404\n",
      "\n",
      "[[83, 6, 3], [4, 119, 23], [3, 37, 126]]\n",
      "Subtyping-Staging-Purity\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.86      0.78       146\n",
      "        luad       0.89      0.68      0.77       166\n",
      "        lusc       0.88      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.82      0.80      0.80       404\n",
      "\n",
      "[[86, 5, 1], [7, 126, 13], [5, 48, 113]]\n",
      "Subtyping-Staging-Purity-FGA\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.88      0.78       146\n",
      "        luad       0.89      0.70      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.84      0.83      0.83       404\n",
      "weighted avg       0.83      0.81      0.81       404\n",
      "\n",
      "[[83, 8, 1], [5, 128, 13], [2, 47, 117]]\n"
     ]
    }
   ],
   "source": [
    "subtyping_reports, subtyping_matrices, results = categorical_task_subtyping(json_files)\n",
    "subtyping_csv = 'calculation\\classification\\subtyping.csv'\n",
    "with open(subtyping_csv, mode='w', newline='\\n') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['Model', 'F1 Score', 'F1 StdDev', 'Precision', 'Recall'])\n",
    "    writer.writeheader()\n",
    "    for row in results:\n",
    "        writer.writerow(row)\n",
    "\n",
    "json_file = 'calculation\\classification\\confusion_subtyping.json'\n",
    "with open(json_file, \"w\") as file:\n",
    "    for i, (model_name, matrix) in enumerate(subtyping_matrices.items()):\n",
    "        if i > 0:\n",
    "            file.write(\"\\n\")\n",
    "        json.dump({model_name: matrix}, file)\n",
    "\n",
    "for classifier_name, json_file in json_files.items():\n",
    "    if 'Subtyping' in classifier_name:\n",
    "        print(classifier_name + \"\\n\")\n",
    "        print(subtyping_reports[classifier_name])\n",
    "        print(subtyping_matrices[classifier_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
