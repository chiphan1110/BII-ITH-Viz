{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_dicts(dir):\n",
    "    json_files = {}\n",
    "    sorted_filenames = sorted(os.listdir(dir), key= lambda x: len(x))\n",
    "    for filename in sorted_filenames:\n",
    "        if filename.endswith(\".json\"):\n",
    "            key_name = filename.replace('.json', '')\n",
    "            file_path = os.path.join(dir, filename)\n",
    "            json_files[key_name] = file_path\n",
    "    \n",
    "    return json_files       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        error = round((pred - label), 3)\n",
    "        errors.append(error)   \n",
    "    return errors\n",
    "\n",
    "def calculate_abs_error(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    abs_error = []\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        error = round(abs(pred - label), 3)\n",
    "        abs_error.append(error)   \n",
    "    return abs_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_task_error(classifiers):\n",
    "    purity_errors = {}\n",
    "    fga_errors = {}\n",
    "\n",
    "    for classifier_name, json_file in classifiers.items():\n",
    "        if 'Purity' in classifier_name:\n",
    "            purity_errors[classifier_name] = calculate_error(json_file, \"purity\")\n",
    "        if 'FGA' in classifier_name:\n",
    "            fga_errors[classifier_name] = calculate_error(json_file, \"FRACTION_GENOME_ALTERED\")\n",
    "\n",
    "    return pd.DataFrame(purity_errors), pd.DataFrame(fga_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_accuracy(json_file, target):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    labels = []\n",
    "    preds = []\n",
    "    unique_labels = set()\n",
    "\n",
    "    for _, value in data.items():\n",
    "        pred = value.get(target, {}).get('pred')\n",
    "        label = value.get(target, {}).get('label')\n",
    "        if pred is None or label is None:\n",
    "            continue\n",
    "        labels.append(label)\n",
    "        preds.append(pred)   \n",
    "        unique_labels.add(label)\n",
    "    \n",
    "    return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_task_staging(classifiers):\n",
    "    staging_labels = []\n",
    "    staging_preds = []\n",
    "    classification_reports = {}\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    for classifier_name, json_file in classifiers.items():\n",
    "\n",
    "        if 'Staging' in classifier_name:\n",
    "            staging_labels, staging_preds = report_accuracy(json_file, \"AJCC_PATHOLOGIC_TUMOR_STAGE_reduced\")\n",
    "            classification_reports[classifier_name] = classification_report(staging_labels, staging_preds, target_names=['Early Stage', 'Late Stage'])\n",
    "            confusion_matrices[classifier_name] = confusion_matrix(staging_labels, staging_preds)\n",
    "            print(classification_reports[classifier_name])\n",
    "            print(confusion_matrices[classifier_name])\n",
    "    return classification_reports, confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_task_subtyping(classifiers):\n",
    "    subtyping_labels = []\n",
    "    subtyping_preds = []\n",
    "    classification_reports = {}\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    for classifier_name, json_file in classifiers.items():\n",
    "        if 'Subtyping' in classifier_name:\n",
    "            print(classifier_name)\n",
    "            subtyping_labels, subtyping_preds = report_accuracy(json_file, \"lung-cancer-subtyping\")\n",
    "\n",
    "            subtyping_labels_counts = pd.Series(list(subtyping_labels)).value_counts()\n",
    "            print(subtyping_labels_counts)\n",
    "\n",
    "            classification_reports[classifier_name] = classification_report(subtyping_labels, subtyping_preds, target_names=['normal', 'luad', 'lusc'])\n",
    "            confusion_matrices[classifier_name] = confusion_matrix(subtyping_labels, subtyping_preds, labels=['normal', 'luad', 'lusc'])\n",
    "            \n",
    "            print(classification_reports[classifier_name])\n",
    "            print(confusion_matrices[classifier_name])\n",
    "            print()\n",
    "    \n",
    "    return classification_reports, confusion_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_dir = 'data'\n",
    "json_files = generate_file_dicts(json_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "purity_errors, fga_errors = numerical_task_error(json_files)\n",
    "purity_errors.to_csv('error/purity.csv', index_label='Sample')\n",
    "fga_errors.to_csv('error/fga.csv', index_label='Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtyping\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.82      0.77       146\n",
      "        luad       0.85      0.72      0.78       166\n",
      "        lusc       0.88      0.95      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[ 87   4   1]\n",
      " [  7 119  20]\n",
      " [  5  42 119]]\n",
      "\n",
      "Subtyping-FGA\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.71      0.82      0.76       146\n",
      "        luad       0.86      0.71      0.78       166\n",
      "        lusc       0.87      0.93      0.90        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.81       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[ 86   5   1]\n",
      " [  8 120  18]\n",
      " [  5  43 118]]\n",
      "\n",
      "Subtyping-Purity\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.77      0.82      0.79       146\n",
      "        luad       0.87      0.77      0.82       166\n",
      "        lusc       0.86      0.96      0.91        92\n",
      "\n",
      "    accuracy                           0.83       404\n",
      "   macro avg       0.83      0.85      0.84       404\n",
      "weighted avg       0.83      0.83      0.83       404\n",
      "\n",
      "[[ 88   3   1]\n",
      " [  9 119  18]\n",
      " [  5  33 128]]\n",
      "\n",
      "Subtyping-Staging\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.77      0.75       146\n",
      "        luad       0.82      0.76      0.79       166\n",
      "        lusc       0.89      0.92      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.82       404\n",
      "weighted avg       0.80      0.80      0.80       404\n",
      "\n",
      "[[ 85   7   0]\n",
      " [  6 113  27]\n",
      " [  4  36 126]]\n",
      "\n",
      "Subtyping-Purity-FGA\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.79      0.75      0.77       146\n",
      "        luad       0.81      0.83      0.82       166\n",
      "        lusc       0.89      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.82       404\n",
      "   macro avg       0.83      0.84      0.83       404\n",
      "weighted avg       0.82      0.82      0.82       404\n",
      "\n",
      "[[ 86   5   1]\n",
      " [  6 109  31]\n",
      " [  5  24 137]]\n",
      "\n",
      "Subtyping-Staging-FGA\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.73      0.82      0.77       146\n",
      "        luad       0.83      0.76      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.83      0.83      0.83       404\n",
      "weighted avg       0.82      0.81      0.81       404\n",
      "\n",
      "[[ 83   6   3]\n",
      " [  4 119  23]\n",
      " [  3  37 126]]\n",
      "\n",
      "Subtyping-Staging-Purity\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.86      0.78       146\n",
      "        luad       0.89      0.68      0.77       166\n",
      "        lusc       0.88      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.82      0.80      0.80       404\n",
      "\n",
      "[[ 86   5   1]\n",
      " [  7 126  13]\n",
      " [  5  48 113]]\n",
      "\n",
      "Subtyping-Staging-Purity-FGA\n",
      "lusc      166\n",
      "luad      146\n",
      "normal     92\n",
      "Name: count, dtype: int64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.88      0.78       146\n",
      "        luad       0.89      0.70      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.84      0.83      0.83       404\n",
      "weighted avg       0.83      0.81      0.81       404\n",
      "\n",
      "[[ 83   8   1]\n",
      " [  5 128  13]\n",
      " [  2  47 117]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subtyping_reports, subtyping_matrices = categorical_task_subtyping(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.82      0.77       146\n",
      "        luad       0.85      0.72      0.78       166\n",
      "        lusc       0.88      0.95      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[119  20   7]\n",
      " [ 42 119   5]\n",
      " [  4   1  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.71      0.82      0.76       146\n",
      "        luad       0.86      0.71      0.78       166\n",
      "        lusc       0.87      0.93      0.90        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.81       404\n",
      "weighted avg       0.81      0.80      0.80       404\n",
      "\n",
      "[[120  18   8]\n",
      " [ 43 118   5]\n",
      " [  5   1  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.77      0.82      0.79       146\n",
      "        luad       0.87      0.77      0.82       166\n",
      "        lusc       0.86      0.96      0.91        92\n",
      "\n",
      "    accuracy                           0.83       404\n",
      "   macro avg       0.83      0.85      0.84       404\n",
      "weighted avg       0.83      0.83      0.83       404\n",
      "\n",
      "[[119  18   9]\n",
      " [ 33 128   5]\n",
      " [  3   1  88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.72      0.77      0.75       146\n",
      "        luad       0.82      0.76      0.79       166\n",
      "        lusc       0.89      0.92      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.81      0.82      0.82       404\n",
      "weighted avg       0.80      0.80      0.80       404\n",
      "\n",
      "[[113  27   6]\n",
      " [ 36 126   4]\n",
      " [  7   0  85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.79      0.75      0.77       146\n",
      "        luad       0.81      0.83      0.82       166\n",
      "        lusc       0.89      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.82       404\n",
      "   macro avg       0.83      0.84      0.83       404\n",
      "weighted avg       0.82      0.82      0.82       404\n",
      "\n",
      "[[109  31   6]\n",
      " [ 24 137   5]\n",
      " [  5   1  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.73      0.82      0.77       146\n",
      "        luad       0.83      0.76      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.83      0.83      0.83       404\n",
      "weighted avg       0.82      0.81      0.81       404\n",
      "\n",
      "[[119  23   4]\n",
      " [ 37 126   3]\n",
      " [  6   3  83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.86      0.78       146\n",
      "        luad       0.89      0.68      0.77       166\n",
      "        lusc       0.88      0.93      0.91        92\n",
      "\n",
      "    accuracy                           0.80       404\n",
      "   macro avg       0.82      0.83      0.82       404\n",
      "weighted avg       0.82      0.80      0.80       404\n",
      "\n",
      "[[126  13   7]\n",
      " [ 48 113   5]\n",
      " [  5   1  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.70      0.88      0.78       146\n",
      "        luad       0.89      0.70      0.79       166\n",
      "        lusc       0.92      0.90      0.91        92\n",
      "\n",
      "    accuracy                           0.81       404\n",
      "   macro avg       0.84      0.83      0.83       404\n",
      "weighted avg       0.83      0.81      0.81       404\n",
      "\n",
      "[[128  13   5]\n",
      " [ 47 117   2]\n",
      " [  8   1  83]]\n"
     ]
    }
   ],
   "source": [
    "staging_reports, staging_matrices = categorical_task_subtyping(json_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
